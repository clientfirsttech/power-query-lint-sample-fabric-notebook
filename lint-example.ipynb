{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Install semantic-link-labs package\n",
    "!pip install semantic-link-labs\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sempy_labs as labs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sempy.fabric as fabric\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "workspace_id = 'your_workspace_id'  # Replace with your actual workspace ID\n",
    "subscription_key = mssparkutils.credentials.getSecret('your_key_vault_address','lint-subscription-key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# API Configuration\n",
    "API_BASE_URL = \"https://api.pqlint.com/v1\"\n",
    "\n",
    "class PQLintApiError(Exception):\n",
    "    pass\n",
    "\n",
    "def invoke_pqlint_api_request(uri, method='GET', headers=None, body=None, content_type='application/json'):\n",
    "    \"\"\"\n",
    "    Helper function to make HTTP requests with proper error handling.\n",
    "    \"\"\"\n",
    "    headers = headers or {}\n",
    "    if content_type:\n",
    "        headers['Content-Type'] = content_type\n",
    "\n",
    "    try:\n",
    "        if method.upper() == 'GET':\n",
    "            response = requests.get(uri, headers=headers)\n",
    "        elif method.upper() == 'POST':\n",
    "            if isinstance(body, (dict, list)):\n",
    "                response = requests.post(uri, headers=headers, data=json.dumps(body))\n",
    "            else:\n",
    "                response = requests.post(uri, headers=headers, data=body)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported HTTP method: {method}\")\n",
    "\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        try:\n",
    "            error_body = response.json()\n",
    "            error_message = f\"HTTP {response.status_code} {response.reason} - {error_body.get('body', str(error_body))}\"\n",
    "        except Exception:\n",
    "            error_message = f\"HTTP {response.status_code} {response.reason} - {http_err}\"\n",
    "        raise PQLintApiError(error_message)\n",
    "    except Exception as err:\n",
    "        raise PQLintApiError(f\"PQLint API Error: {str(err)}\")\n",
    "\n",
    "def get_linting_rules():\n",
    "    \"\"\"\n",
    "    Retrieves all linting rules from the PQLint API.\n",
    "    \"\"\"\n",
    "    uri = f\"{API_BASE_URL}/lint/rules\"\n",
    "    return invoke_pqlint_api_request(uri, method='GET')\n",
    "\n",
    "def invoke_code_linting(code, subscription_key, rules=None, severity=None, format='pq'):\n",
    "    \"\"\"\n",
    "    Lints Power Query M code or TMDL code using the PQLint API.\n",
    "    \"\"\"\n",
    "    if not code.strip():\n",
    "        raise ValueError(\"Code parameter cannot be null or empty\")\n",
    "    if not subscription_key.strip():\n",
    "        raise ValueError(\"SubscriptionKey parameter cannot be null or empty\")\n",
    "\n",
    "    request_body = {\n",
    "        \"code\": code\n",
    "    }\n",
    "\n",
    "    if rules:\n",
    "        request_body[\"rules\"] = rules\n",
    "\n",
    "    options = {}\n",
    "    if severity:\n",
    "        options[\"severity\"] = severity\n",
    "    if format:\n",
    "        options[\"format\"] = format\n",
    "\n",
    "    if options:\n",
    "        request_body[\"options\"] = options\n",
    "\n",
    "    encoded_key = urlencode({\"subscription-key\": subscription_key})\n",
    "    uri = f\"{API_BASE_URL}/pq/lint?{encoded_key}\"\n",
    "\n",
    "    return invoke_pqlint_api_request(uri, method='POST', body=request_body)\n",
    "\n",
    "\n",
    "def lint_result_to_df(lint_result, dataset_name, table_name, partition_name):\n",
    "    \"\"\"\n",
    "    Converts lint_result (JSON/dict/list) to a DataFrame and adds DatasetName, TableName, and PartitionName columns.\n",
    "\n",
    "    Args:\n",
    "        lint_result (dict or list): The lint result JSON or list of rule violations.\n",
    "        dataset_name (str): The dataset name to add.\n",
    "        table_name (str): The table name to add.\n",
    "        partition_name (str): The partition name to add.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Expanded DataFrame with relevant columns including DatasetName, TableName, PartitionName.\n",
    "    \"\"\"\n",
    "    if isinstance(lint_result, list):\n",
    "        df = pd.json_normalize(\n",
    "            lint_result,\n",
    "            sep='_',\n",
    "            meta=['ID', 'Name', 'Category', 'Description', 'Severity'],\n",
    "            errors='ignore'\n",
    "        )\n",
    "    elif isinstance(lint_result, dict):\n",
    "        df = pd.json_normalize([lint_result], sep='_')\n",
    "    else:\n",
    "        raise ValueError(\"lint_result must be a dict or list\")\n",
    "\n",
    "    # Optional: Flatten References list to semicolon-separated string\n",
    "    if 'References' in df.columns:\n",
    "        df['References'] = df['References'].apply(lambda refs: '; '.join(\n",
    "            f\"{r.get('Description', '')} ({r.get('Link', '')})\" for r in refs\n",
    "        ) if isinstance(refs, list) else None)\n",
    "\n",
    "    # Rename or fill missing columns if necessary\n",
    "    if 'ErrorInformation_errorLocation_positionStart_lineNumber' not in df.columns:\n",
    "        df['ErrorInformation_errorLocation_positionStart_lineNumber'] = None\n",
    "\n",
    "    # Add Dataset/Table/Partition metadata\n",
    "    df['DatasetName'] = dataset_name\n",
    "    df['TableName'] = table_name\n",
    "    df['PartitionName'] = partition_name\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Retrieve all semantic models/datasets and run linter on Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_datasets = fabric.list_datasets(workspace_id)\n",
    "\n",
    "# Remove rows where the 'Dataset Name' is 'Usage Metrics Report'\n",
    "df_datasets = df_datasets[df_datasets['Dataset Name'] != 'Usage Metrics Report']\n",
    "\n",
    "print(df_datasets['Dataset ID'])\n",
    "from sempy_labs.tom import connect_semantic_model\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for d in df_datasets.to_dict(orient=\"records\"):\n",
    "    with connect_semantic_model(dataset=d['Dataset ID'], workspace=workspace_id, readonly=True) as tom:\n",
    "        for t in tom.model.Tables:\n",
    "            for c in t.Partitions:\n",
    "                # Skip partitions like 'LocalDateTable_...'\n",
    "                if c.Name.startswith(\"LocalDateTable_\"):\n",
    "                    continue\n",
    "\n",
    "                print(f\"Linting '{t.Name}'[{c.Name}] in dataset '{d['Dataset Name']}'\")\n",
    "\n",
    "                try:\n",
    "                    lint_result = invoke_code_linting(\n",
    "                        code=c.Source.Expression,\n",
    "                        subscription_key=subscription_key\n",
    "                    )\n",
    "\n",
    "                    df_temp = lint_result_to_df(\n",
    "                        lint_result,\n",
    "                        dataset_name=d['Dataset Name'],\n",
    "                        table_name=t.Name,\n",
    "                        partition_name=c.Name\n",
    "                    )\n",
    "                    # Output results\n",
    "                    display(df_temp)\n",
    "\n",
    "                    results.append(df_temp)\n",
    "\n",
    "                except Exception as lint_err:\n",
    "                    print(f\"[ERROR] Linting failed for {d['Dataset Name']} -> {t.Name} -> {c.Name}: {lint_err}\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "df_all = pd.concat(results, ignore_index=True)\n",
    "\n",
    "#print(df_all)\n",
    "import pandas as pd\n",
    "\n",
    "# After creating df_all as shown previously\n",
    "\n",
    "# Example aggregation: count of categories per TableName and Partition\n",
    "# Assuming your DataFrame has columns: 'TableName', 'Partition', 'Category'\n",
    "\n",
    "agg_df = (\n",
    "    df_all\n",
    "    .groupby(['DatasetName', 'TableName', 'PartitionName', 'Category'])\n",
    "    .size()\n",
    "    .reset_index(name='Count')\n",
    ")\n",
    "\n",
    "print(tabulate(agg_df, headers='keys', tablefmt='github', showindex=False))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
